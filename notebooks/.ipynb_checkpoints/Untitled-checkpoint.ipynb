{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbdc99-d6b5-4eaa-bce6-0f2814ecd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "EPSILON = 10-6  # 1 micrometer (um)\n",
    "X0_MIN = 0\n",
    "X0_MAX = 1\n",
    "\n",
    "D_MODEL = 512\n",
    "\n",
    "\"\"\"\n",
    "Note that we might be able to skip the input encoding step if we use the spacetime vector itself as\n",
    "the embedding. This would make our vocabulary size infinite, and i'm not sure that we would be able \n",
    "to encode that as output tokens, since it does a lookup against the vocabulary.\n",
    "\n",
    "We might be able to use the output embeddings, though, to represent elements of spacetime, avoiding\n",
    "the idea of a vocabulary altogether.\n",
    "\"\"\"\n",
    "VOCAB_SIZE = (X0_MAX - X0_MIN) / EPSILON\n",
    "EMBEDDING_DIM = D_MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_embedding = torch.nn.Embedding(VOCAB_SIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
