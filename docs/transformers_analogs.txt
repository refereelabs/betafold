How does the hamiltonian of an increment of space (NOT a particle in particular) map to some part of the transformer model

The hamiltonian of an increment of space would be analogous to the semantics of a word in transformers.
These are defined by the vector embedding matrices. We should be able to map the hamiltonian (describe by the involved variables)
to an embedding vector to describe an increment of space


How are we building training data? (use an existing physics engine, generate 'correct' answers)

We could start training in 1D/2D on simple cases.


Describe what d_model represents here

d_model represents the dimensions the model is allowed to work with. In transformers, the model uses these dimensions
to describe the words/tokens. In our case, the model would determine the 'meaning' (potential?) of an increment of space
using d_model dimensions.


Define the analog of query, key, and value vectors between transformers and our representation of physical space

In transformers, the query, key and value matrices are used to interact with the model. The query is built using the
embedded tokens and the query matrix. 
In our case, the query would be the initial hamiltonians (or states) for all increments in the considered space.


How does attention work in this context? (it should relate to the proximity of regions of space, 
since the closeness of two regions governs the impact of one on the other in the same way that the meaning 
and location of words do in an LLM) position in the sentence should relate to position in space

Attention calculates the interactions between elements of the models, for words it's 
semantics (token embedding) + position in the sentence (positional encoding)
In our case, the positional encoding would encode the position of the increment of space, and the embedding the hamiltonian

describe how we represent time not as a loop parameter, but as a dimension of spacetime (see some discussion of 4-momentum and/or the energy-momentum tensor AKA stress-energy tensor)